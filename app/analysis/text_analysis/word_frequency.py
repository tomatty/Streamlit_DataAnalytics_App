"""
Word frequency analysis module with Japanese support.
"""
import pandas as pd
import streamlit as st
import plotly.express as px
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

try:
    from janome.tokenizer import Tokenizer
    JANOME_AVAILABLE = True
except ImportError:
    JANOME_AVAILABLE = False

_JP_FONT_CANDIDATES = [
    "Noto Sans CJK JP",
    "Noto Sans JP",
    "IPAexGothic",
    "IPAGothic",
    "TakaoGothic",
    "VL Gothic",
    "Hiragino Sans",
    "Hiragino Kaku Gothic ProN",
    "Yu Gothic",
]


def _find_jp_font() -> str | None:
    """
    Find an available Japanese font on the system.

    Returns the path to the font file if found, otherwise None.
    Works across macOS, Linux (including Docker), and Windows.
    """
    try:
        fm._load_fontmanager(try_read_cache=False)
    except Exception:
        pass

    available_fonts = {f.name: f.fname for f in fm.fontManager.ttflist}

    for candidate in _JP_FONT_CANDIDATES:
        if candidate in available_fonts:
            return available_fonts[candidate]

    return None


def _parse_rgb_color(color: str) -> tuple[float, float, float]:
    """
    Convert WordCloud's 'rgb(r, g, b)' string to matplotlib-compatible tuple.

    Args:
        color: Color string in format 'rgb(r, g, b)' where r, g, b are 0-255

    Returns:
        Tuple of (r, g, b) normalized to 0-1 range
    """
    import re
    match = re.match(r'rgb\((\d+),\s*(\d+),\s*(\d+)\)', color)
    if match:
        r, g, b = map(int, match.groups())
        return (r / 255.0, g / 255.0, b / 255.0)
    # Fallback to black if parsing fails
    return (0.0, 0.0, 0.0)


def _render_wordcloud_matplotlib(wc: WordCloud, font_path: str | None) -> plt.Figure:
    """
    Re-render WordCloud layout using matplotlib's FreeType renderer.

    WordCloud uses PIL internally, which cannot always access Japanese glyphs
    from macOS .ttc files. This function takes the already-computed layout
    (word positions, sizes, orientations) and draws each word with matplotlib,
    which handles Japanese fonts via FreeType correctly.
    """
    WC_WIDTH = wc.width
    WC_HEIGHT = wc.height

    font_props = fm.FontProperties(fname=font_path) if font_path else None
    if font_path:
        fm.fontManager.addfont(font_path)

    fig, ax = plt.subplots(figsize=(10, 5))
    fig.patch.set_facecolor(wc.background_color)
    ax.set_facecolor(wc.background_color)
    # Match PIL coordinate system: origin top-left, y increases downward
    ax.set_xlim(0, WC_WIDTH)
    ax.set_ylim(WC_HEIGHT, 0)
    ax.axis("off")
    plt.tight_layout(pad=0)

    for (word, _), font_size, (x, y), orientation, color in wc.layout_:
        rot = 90 if orientation else 0
        # Convert PIL/WordCloud color format to matplotlib format
        mpl_color = _parse_rgb_color(color) if isinstance(color, str) and color.startswith('rgb(') else color
        ax.text(
            x, y, word,
            fontsize=font_size * 0.75,
            fontproperties=font_props,
            color=mpl_color,
            rotation=rot,
            ha="left",
            va="top",
        )

    return fig


def show_word_frequency_analysis(df: pd.DataFrame):
    """Display word frequency analysis interface."""
    st.subheader("ğŸ“ å˜èªé »åº¦åˆ†æ")

    with st.expander("ğŸ“– ä¸€èˆ¬çš„ãªåˆ†ææ‰‹é †", expanded=False):
        st.markdown(
            """
### å˜èªé »åº¦åˆ†æã®åŸºæœ¬çš„ãªæµã‚Œ

**1. ç›®çš„ã®æ˜ç¢ºåŒ–**
- é »å‡ºå˜èªã®æŠŠæ¡: ã©ã®å˜èªãŒå¤šãä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹
- ãƒˆãƒ¬ãƒ³ãƒ‰ã®ç™ºè¦‹: è©±é¡Œã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®š
- é¡§å®¢ã®å£°ã®åˆ†æ: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã®è‡ªç”±è¨˜è¿°ã‹ã‚‰å‚¾å‘ã‚’æŠŠæ¡
- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç‰¹å¾´æŠ½å‡º: æ–‡æ›¸ã®ç‰¹å¾´çš„ãªå˜èªã‚’ç™ºè¦‹

**2. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™**
- **ãƒ‡ãƒ¼ã‚¿å½¢å¼**:
  ```
  | ID | ãƒ†ã‚­ã‚¹ãƒˆåˆ—                          |
  |----|-----------------------------------|
  | 1  | ã“ã®å•†å“ã¯ä½¿ã„ã‚„ã™ãæº€è¶³ã—ã¦ã„ã¾ã™   |
  | 2  | ã‚µãƒãƒ¼ãƒˆãŒè¦ªåˆ‡ã§åŠ©ã‹ã‚Šã¾ã—ãŸ         |
  | 3  | ä¾¡æ ¼ãŒé«˜ã„ã§ã™ãŒå“è³ªã¯è‰¯ã„ã§ã™       |
  ```
- ãƒ†ã‚­ã‚¹ãƒˆåˆ—ï¼ˆobjectå‹ï¼‰ãŒå¿…è¦
- 1è¡Œã«1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã‚³ãƒ¡ãƒ³ãƒˆã€å›ç­”ãªã©ï¼‰
- æœ€ä½ã§ã‚‚10ä»¶ä»¥ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆãŒæœ›ã¾ã—ã„

**3. è¨€èªã®é¸æŠ**
- **æ—¥æœ¬èª**: å½¢æ…‹ç´ è§£æï¼ˆJanomeï¼‰ã§å˜èªåˆ†å‰²
  - ã€Œä½¿ã„ã‚„ã™ã„ã€â†’ã€Œä½¿ã„ã€ã€Œã‚„ã™ã„ã€ã«åˆ†å‰²
  - å“è©ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆåè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ãªã©ï¼‰
- **è‹±èª**: ç©ºç™½ã§å˜èªåˆ†å‰²
  - ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»ï¼ˆthe, is, a ãªã©ï¼‰

**4. å‰å‡¦ç†**
- **ãƒã‚¤ã‚ºé™¤å»**: URLã€è¨˜å·ã€æ•°å­—ãªã©
- **ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»**: æ„å‘³ã®è–„ã„å˜èªï¼ˆã€Œã“ã‚Œã€ã€Œãã®ã€ãªã©ï¼‰
- **æ­£è¦åŒ–**: å¤§æ–‡å­—å°æ–‡å­—ã®çµ±ä¸€
- **èªå¹¹æŠ½å‡º**: æ´»ç”¨å½¢ã®çµ±ä¸€ï¼ˆè‹±èªã®å ´åˆï¼‰

**5. é »åº¦ã®é›†è¨ˆ**
- å˜èªã”ã¨ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
- ä¸Šä½Nä»¶ã‚’æŠ½å‡ºï¼ˆé€šå¸¸20-50ä»¶ï¼‰
- å‡ºç¾æ–‡æ›¸æ•°ã‚‚è€ƒæ…®ï¼ˆTF-IDFã‚’ä½¿ã†ã¨ã‚ˆã‚Šé«˜åº¦ï¼‰

**6. å¯è¦–åŒ–**
- **æ£’ã‚°ãƒ©ãƒ•**: ä¸Šä½å˜èªã®é »åº¦ã‚’æ¯”è¼ƒ
- **ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰**: ç›´æ„Ÿçš„ã«é »å‡ºå˜èªã‚’è¡¨ç¤º
  - å¤§ãã„æ–‡å­—ã»ã©é »åº¦ãŒé«˜ã„
  - è¦–è¦šçš„ãªã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãŒå¼·ã„

**7. çµæœã®è§£é‡ˆ**
- é »å‡ºå˜èªã‹ã‚‰ä¸»è¦ãªãƒ†ãƒ¼ãƒã‚’æŠŠæ¡
- æ„å¤–ãªå˜èªã®ç™ºè¦‹ï¼ˆæ–°ã—ã„ãƒ‹ãƒ¼ã‚ºã€å•é¡Œç‚¹ï¼‰
- ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ãªå˜èªã®åˆ†å¸ƒ
- æ™‚ç³»åˆ—ã§ã®å¤‰åŒ–ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼‰

**8. æ³¨æ„ç‚¹**
- é »åº¦ã ã‘ã§ã¯æ–‡è„ˆãŒã‚ã‹ã‚‰ãªã„
- ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®šã§çµæœãŒå¤‰ã‚ã‚‹
- åŒç¾©èªãŒåˆ¥å˜èªã¨ã—ã¦æ‰±ã‚ã‚Œã‚‹
- çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã§ã¯æ„å‘³ã®ã‚ã‚‹åˆ†æãŒé›£ã—ã„
            """
        )

    text_cols = df.select_dtypes(include=["object"]).columns.tolist()

    if not text_cols:
        st.warning("ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    col1, col2 = st.columns(2)
    with col1:
        text_col = st.selectbox("ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’é¸æŠ", text_cols)
    with col2:
        language = st.selectbox("è¨€èª", ["æ—¥æœ¬èª", "è‹±èª"])

    if language == "æ—¥æœ¬èª" and not JANOME_AVAILABLE:
        st.error("JanomeãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install janome`ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    max_words = st.slider("åˆ†æã™ã‚‹å˜èªæ•°ï¼ˆãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šé™ï¼‰", min_value=10, max_value=100, value=50)

    if st.button("å˜èªé »åº¦åˆ†æã‚’å®Ÿè¡Œ", type="primary"):
        try:
            texts = df[text_col].dropna()

            if len(texts) == 0:
                st.error("æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return

            # Tokenize
            if language == "æ—¥æœ¬èª":
                tokenizer = Tokenizer()
                words = []
                for text in texts:
                    tokens = tokenizer.tokenize(str(text))
                    words.extend([
                        token.base_form for token in tokens
                        if token.part_of_speech.split(",")[0] in ["åè©", "å‹•è©", "å½¢å®¹è©"]
                    ])
            else:
                words = []
                for text in texts:
                    words.extend(str(text).lower().split())

            if len(words) == 0:
                st.error("æœ‰åŠ¹ãªå˜èªãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            word_counts = Counter(words)
            top_words = word_counts.most_common(max_words)
            freq_df = pd.DataFrame(top_words, columns=["å˜èª", "å‡ºç¾å›æ•°"])
            freq_df["é †ä½"] = range(1, len(freq_df) + 1)
            freq_df = freq_df[["é †ä½", "å˜èª", "å‡ºç¾å›æ•°"]]

            # Persist results so slider reruns don't lose them
            st.session_state["wf_freq_df"] = freq_df
            st.session_state["wf_word_counts"] = dict(word_counts)
            st.session_state["wf_total_words"] = len(words)
            st.session_state["wf_max_words"] = max_words
            st.session_state["wf_language"] = language

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    # Render results (survives slider reruns via session_state)
    if "wf_freq_df" not in st.session_state:
        return

    freq_df = st.session_state["wf_freq_df"]
    word_counts = st.session_state["wf_word_counts"]
    total_words = st.session_state["wf_total_words"]
    saved_max_words = st.session_state["wf_max_words"]
    saved_language = st.session_state["wf_language"]

    st.success(f"å˜èªé »åº¦åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼ï¼ˆç·å˜èªæ•°: {total_words}ï¼‰")

    # Frequency table
    st.markdown("### é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    st.dataframe(freq_df, use_container_width=True)

    # Horizontal bar chart with display-count control
    st.markdown("### å˜èªé »åº¦ã‚°ãƒ©ãƒ•")
    chart_n = st.slider(
        "ã‚°ãƒ©ãƒ•è¡¨ç¤ºæ•°",
        min_value=5,
        max_value=min(saved_max_words, len(freq_df)),
        value=min(20, len(freq_df)),
        key="chart_n_slider",
    )
    chart_df = freq_df.head(chart_n).sort_values("å‡ºç¾å›æ•°", ascending=True)
    fig = px.bar(
        chart_df,
        x="å‡ºç¾å›æ•°",
        y="å˜èª",
        orientation="h",
        title=f"Top {chart_n} å˜èª",
    )
    fig.update_layout(yaxis={"tickfont": {"size": 11}})
    st.plotly_chart(fig, use_container_width=True)

    # Word cloud
    st.markdown("### ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
    try:
        font_path = _find_jp_font() if saved_language == "æ—¥æœ¬èª" else None
        wc = WordCloud(
            width=800,
            height=400,
            background_color="white",
            font_path=font_path,
            max_words=saved_max_words,
        ).generate_from_frequencies(word_counts)

        if saved_language == "æ—¥æœ¬èª" and font_path:
            fig_wc = _render_wordcloud_matplotlib(wc, font_path)
        else:
            fig_wc, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wc, interpolation="bilinear")
            ax.axis("off")

        st.pyplot(fig_wc)
        plt.close(fig_wc)
    except Exception as e:
        st.warning(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    # Download
    csv = freq_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        label="å˜èªé »åº¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="word_frequency.csv",
        mime="text/csv",
    )
