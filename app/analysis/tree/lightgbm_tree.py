"""
LightGBM decision tree analysis module.
Supports both classification and regression tasks.
"""
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
)
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder

try:
    import graphviz
    import lightgbm as lgb
except ImportError:
    lgb = None
    graphviz = None

_SS = "lgb_results"  # session_state key


def _get_tree_depth(node: dict) -> int:
    """Recursively compute the depth of a LightGBM tree node."""
    if "leaf_index" in node:
        return 0
    return 1 + max(
        _get_tree_depth(node["left_child"]),
        _get_tree_depth(node["right_child"]),
    )


def _build_depth_limited_digraph(
    tree_dict: dict,
    max_display_depth: int,
    feature_names: list,
    precision: int = 3,
) -> str:
    """Build a graphviz source string from a LightGBM tree dict, limited to max_display_depth."""
    graph = graphviz.Digraph(
        graph_attr={"rankdir": "TB", "fontsize": "12"},
        node_attr={"fontsize": "11"},
        edge_attr={"fontsize": "10"},
    )
    counter = [0]

    def add_node(node: dict, parent_id: str | None, edge_label: str, depth: int) -> None:
        node_id = str(counter[0])
        counter[0] += 1

        is_leaf = "leaf_index" in node

        if is_leaf or depth >= max_display_depth:
            if is_leaf:
                value = node.get("leaf_value", 0)
                count = node.get("leaf_count", 0)
                label = f"leaf: {value:.{precision}f}\ncount: {count}"
            else:
                count = node.get("internal_count", 0)
                label = f"...\ncount: {count}"
            graph.node(node_id, label=label, shape="ellipse", style="filled", fillcolor="#ffffcc")
        else:
            feat = node.get("split_feature", "?")
            if feature_names and isinstance(feat, int) and feat < len(feature_names):
                feat = feature_names[feat]
            threshold = node.get("threshold", "?")
            try:
                threshold_str = f"{float(threshold):.{precision}f}"
            except (ValueError, TypeError):
                threshold_str = str(threshold)
            gain = node.get("split_gain", 0)
            count = node.get("internal_count", 0)
            label = f"{feat} <= {threshold_str}\ngain: {gain:.{precision}f}\ncount: {count}"
            graph.node(node_id, label=label, shape="box", style="filled", fillcolor="#dae8fc")

        if parent_id is not None:
            graph.edge(parent_id, node_id, label=edge_label)

        if not is_leaf and depth < max_display_depth:
            add_node(node["left_child"], node_id, "yes", depth + 1)
            add_node(node["right_child"], node_id, "no", depth + 1)

    add_node(tree_dict.get("tree_structure", {}), None, "", 0)
    return graph.source


def show_lightgbm_tree(df: pd.DataFrame):
    """Display LightGBM analysis interface."""
    st.subheader("âš¡ LightGBM æ±ºå®šæœ¨åˆ†æ")

    if lgb is None:
        st.error("lightgbmãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install lightgbm`ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    all_cols = df.columns.tolist()

    if len(all_cols) < 2:
        st.warning("LightGBMåˆ†æã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®åˆ—ãŒå¿…è¦ã§ã™ã€‚")
        return

    col1, col2 = st.columns(2)
    with col1:
        target_col = st.selectbox("ç›®çš„å¤‰æ•°ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰", all_cols, key="lgb_target")
    with col2:
        task_type = st.radio(
            "ã‚¿ã‚¹ã‚¯ç¨®åˆ¥",
            ["åˆ†é¡ï¼ˆClassificationï¼‰", "å›å¸°ï¼ˆRegressionï¼‰"],
            key="lgb_task",
            horizontal=True,
        )

    feature_cols = st.multiselect(
        "èª¬æ˜å¤‰æ•°ï¼ˆç‰¹å¾´é‡ï¼‰",
        [c for c in numeric_cols if c != target_col],
        default=[c for c in numeric_cols if c != target_col][:min(5, len(numeric_cols))],
        key="lgb_features",
    )

    if len(feature_cols) < 1:
        st.info("å°‘ãªãã¨ã‚‚1ã¤ã®èª¬æ˜å¤‰æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        return

    st.markdown("#### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
    p_col1, p_col2, p_col3, p_col4 = st.columns(4)
    with p_col1:
        n_estimators = st.slider("æœ¨ã®æœ¬æ•°ï¼ˆn_estimatorsï¼‰", 50, 500, 100, step=50, key="lgb_n")
    with p_col2:
        learning_rate = st.select_slider(
            "å­¦ç¿’ç‡ï¼ˆlearning_rateï¼‰",
            options=[0.01, 0.05, 0.1, 0.2, 0.3],
            value=0.1,
            key="lgb_lr",
        )
    with p_col3:
        max_depth = st.slider("æœ€å¤§æ·±ã•ï¼ˆmax_depthï¼‰", -1, 10, 4, key="lgb_depth",
                              help="-1 ã¯ç„¡åˆ¶é™")
    with p_col4:
        test_size = st.slider("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‰²åˆ", 0.1, 0.4, 0.2, step=0.05, key="lgb_test")

    num_leaves = st.slider("è‘‰ã®æœ€å¤§æ•°ï¼ˆnum_leavesï¼‰", 4, 128, 31, key="lgb_leaves")

    if st.button("LightGBMåˆ†æã‚’å®Ÿè¡Œ", type="primary", key="lgb_run"):
        try:
            data_subset = df[feature_cols + [target_col]].dropna()

            if len(data_subset) < 10:
                st.error("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½10è¡Œå¿…è¦ï¼‰ã€‚")
                return

            X = data_subset[feature_cols]
            y_raw = data_subset[target_col]
            is_regression = task_type.startswith("å›å¸°")

            le = None
            if is_regression:
                y = y_raw.astype(float)
                model = lgb.LGBMRegressor(
                    n_estimators=n_estimators,
                    learning_rate=learning_rate,
                    max_depth=max_depth,
                    num_leaves=num_leaves,
                    random_state=42,
                    verbose=-1,
                )
                scoring = "r2"
            else:
                le = LabelEncoder()
                y = le.fit_transform(y_raw.astype(str))
                n_classes = len(le.classes_)
                objective = "multiclass" if n_classes > 2 else "binary"
                model = lgb.LGBMClassifier(
                    n_estimators=n_estimators,
                    learning_rate=learning_rate,
                    max_depth=max_depth,
                    num_leaves=num_leaves,
                    objective=objective,
                    random_state=42,
                    verbose=-1,
                )
                scoring = "accuracy"

            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42
            )

            # Fit with eval for learning curves
            evals_result: dict = {}
            eval_metric = "rmse" if is_regression else (
                "binary_logloss" if len(np.unique(y)) == 2 else "multi_logloss"
            )
            model.fit(
                X_train, y_train,
                eval_set=[(X_train, y_train), (X_test, y_test)],
                eval_metric=eval_metric,
                callbacks=[lgb.log_evaluation(period=-1), lgb.record_evaluation(evals_result)],
            )
            y_pred = model.predict(X_test)

            # Cross-validation
            if is_regression:
                n_cv = min(5, len(y) // 2)
            else:
                min_class_count = int(np.bincount(y).min())
                n_cv = min(5, min_class_count)
            cv_scores = cross_val_score(model, X, y, cv=n_cv, scoring=scoring) if n_cv >= 2 else None

            # Metrics
            if is_regression:
                metrics = {
                    "r2": float(r2_score(y_test, y_pred)),
                    "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
                    "mae": float(mean_absolute_error(y_test, y_pred)),
                    "cv_mean": float(cv_scores.mean()) if cv_scores is not None else None,
                    "cv_std": float(cv_scores.std()) if cv_scores is not None else None,
                }
            else:
                metrics = {
                    "acc": float(accuracy_score(y_test, y_pred)),
                    "cv_mean": float(cv_scores.mean()) if cv_scores is not None else None,
                    "cv_std": float(cv_scores.std()) if cv_scores is not None else None,
                }

            st.session_state[_SS] = {
                "model": model,
                "feature_cols": feature_cols,
                "is_regression": is_regression,
                "n_estimators": n_estimators,
                "le": le,
                "y_test": y_test,
                "y_pred": y_pred,
                "X_train": X_train,
                "X_test": X_test,
                "y_train": y_train,
                "metrics": metrics,
                "evals_result": evals_result,
                "eval_metric_key": eval_metric,
            }

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return

    # ---- Render results from session_state ----
    if _SS not in st.session_state:
        return

    res = st.session_state[_SS]
    model = res["model"]
    feature_cols = res["feature_cols"]
    is_regression = res["is_regression"]
    n_estimators = res["n_estimators"]
    le = res["le"]
    y_test = res["y_test"]
    y_pred = res["y_pred"]
    X_train = res["X_train"]
    X_test = res["X_test"]
    y_train = res["y_train"]
    metrics = res["metrics"]
    evals_result = res["evals_result"]

    st.success("LightGBMåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    # --- Metrics ---
    st.markdown("### ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™")

    if is_regression:
        r2, rmse, mae = metrics["r2"], metrics["rmse"], metrics["mae"]
        cv_str = f"{metrics['cv_mean']:.4f}" if metrics["cv_mean"] is not None else "N/A"
        m_cols = st.columns(4)
        with m_cols[0]:
            with st.container(border=True):
                st.metric("RÂ²ï¼ˆãƒ†ã‚¹ãƒˆï¼‰", f"{r2:.4f}")
        with m_cols[1]:
            with st.container(border=True):
                st.metric("RMSE", f"{rmse:.4f}")
        with m_cols[2]:
            with st.container(border=True):
                st.metric("MAE", f"{mae:.4f}")
        with m_cols[3]:
            with st.container(border=True):
                st.metric("CV RÂ²ï¼ˆå¹³å‡ï¼‰", cv_str)
        if metrics["cv_mean"] is None:
            st.warning("ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚äº¤å·®æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        with st.expander("ğŸ“– å›å¸°æŒ‡æ¨™ã®è§£é‡ˆ"):
            cv_detail = f"CV RÂ²={metrics['cv_mean']:.4f}Â±{metrics['cv_std']:.4f}" if metrics["cv_mean"] is not None else "CV: ã‚¹ã‚­ãƒƒãƒ—"
            st.markdown(f"""
**RÂ²**: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ±ºå®šä¿‚æ•°ã€‚1ã«è¿‘ã„ã»ã©è‰¯ã„äºˆæ¸¬ã€‚

**RMSE / MAE**: äºˆæ¸¬èª¤å·®ã€‚ç›®çš„å¤‰æ•°ã¨åŒã˜å˜ä½ã§è§£é‡ˆã§ãã‚‹ã€‚

**CV RÂ²**: äº¤å·®æ¤œè¨¼ã®å¹³å‡RÂ²ã€‚éå­¦ç¿’ã®æ¤œå‡ºã«ä½¿ç”¨ã€‚

ç¾åœ¨ã®å€¤: RÂ²={r2:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, {cv_detail}
            """)
    else:
        acc = metrics["acc"]
        cv_mean_str = f"{metrics['cv_mean']:.4f}" if metrics["cv_mean"] is not None else "N/A"
        cv_std_str = f"{metrics['cv_std']:.4f}" if metrics["cv_std"] is not None else "N/A"
        m_cols = st.columns(3)
        with m_cols[0]:
            with st.container(border=True):
                st.metric("æ­£è§£ç‡ï¼ˆãƒ†ã‚¹ãƒˆï¼‰", f"{acc:.4f}")
        with m_cols[1]:
            with st.container(border=True):
                st.metric("CV æ­£è§£ç‡ï¼ˆå¹³å‡ï¼‰", cv_mean_str)
        with m_cols[2]:
            with st.container(border=True):
                st.metric("CV æ¨™æº–åå·®", cv_std_str)
        if metrics["cv_mean"] is None:
            st.warning("ä¸€éƒ¨ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚äº¤å·®æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

        st.markdown("#### åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ")
        report_dict = classification_report(
            y_test, y_pred,
            target_names=le.classes_.astype(str),
            output_dict=True,
        )
        report_df = pd.DataFrame(report_dict).T
        st.dataframe(
            report_df.style.format("{:.3f}", subset=["precision", "recall", "f1-score"]),
            use_container_width=True,
        )

        st.markdown("#### æ··åŒè¡Œåˆ—")
        cm = confusion_matrix(y_test, y_pred)
        fig_cm = px.imshow(
            cm,
            labels=dict(x="äºˆæ¸¬ã‚¯ãƒ©ã‚¹", y="å®Ÿéš›ã®ã‚¯ãƒ©ã‚¹", color="ä»¶æ•°"),
            x=le.classes_.astype(str),
            y=le.classes_.astype(str),
            text_auto=True,
            color_continuous_scale="Blues",
        )
        fig_cm.update_layout(title="æ··åŒè¡Œåˆ—")
        st.plotly_chart(fig_cm, use_container_width=True)

        with st.expander("ğŸ“– åˆ†é¡æŒ‡æ¨™ã®è§£é‡ˆ"):
            cv_detail = f"CV={metrics['cv_mean']:.4f}Â±{metrics['cv_std']:.4f}" if metrics["cv_mean"] is not None else "CV: ã‚¹ã‚­ãƒƒãƒ—"
            st.markdown(f"""
**æ­£è§£ç‡ï¼ˆAccuracyï¼‰**: å…¨äºˆæ¸¬ã®ã†ã¡æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸå‰²åˆã€‚

**Precision / Recall / F1**: ã‚¯ãƒ©ã‚¹ã”ã¨ã®ç²¾åº¦ãƒ»å†ç¾ç‡ãƒ»èª¿å’Œå¹³å‡ã€‚

**CV æ­£è§£ç‡**: äº¤å·®æ¤œè¨¼ã®å¹³å‡æ­£è§£ç‡ã€‚éå­¦ç¿’ã®æ¤œå‡ºã«ä½¿ç”¨ã€‚

ç¾åœ¨ã®å€¤: æ­£è§£ç‡={acc:.4f}, {cv_detail}
            """)

    # --- Feature Importance ---
    st.markdown("### ç‰¹å¾´é‡ã®é‡è¦åº¦")
    imp_tab1, imp_tab2 = st.tabs(["gainï¼ˆæƒ…å ±åˆ©å¾—ï¼‰", "splitï¼ˆåˆ†å‰²å›æ•°ï¼‰"])
    booster = model.booster_

    with imp_tab1:
        imp_gain = booster.feature_importance(importance_type="gain")
        imp_df_gain = pd.DataFrame({
            "ç‰¹å¾´é‡": feature_cols, "é‡è¦åº¦ï¼ˆgainï¼‰": imp_gain,
        }).sort_values("é‡è¦åº¦ï¼ˆgainï¼‰", ascending=False)
        fig_gain = px.bar(
            imp_df_gain, x="é‡è¦åº¦ï¼ˆgainï¼‰", y="ç‰¹å¾´é‡", orientation="h",
            title="ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆgain: æƒ…å ±åˆ©å¾—ã®åˆè¨ˆï¼‰",
            color="é‡è¦åº¦ï¼ˆgainï¼‰", color_continuous_scale="Greens",
        )
        fig_gain.update_layout(showlegend=False, yaxis={"categoryorder": "total ascending"})
        st.plotly_chart(fig_gain, use_container_width=True)

    with imp_tab2:
        imp_split = booster.feature_importance(importance_type="split")
        imp_df_split = pd.DataFrame({
            "ç‰¹å¾´é‡": feature_cols, "é‡è¦åº¦ï¼ˆsplitï¼‰": imp_split,
        }).sort_values("é‡è¦åº¦ï¼ˆsplitï¼‰", ascending=False)
        fig_split = px.bar(
            imp_df_split, x="é‡è¦åº¦ï¼ˆsplitï¼‰", y="ç‰¹å¾´é‡", orientation="h",
            title="ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆsplit: ä½¿ç”¨ã•ã‚ŒãŸåˆ†å‰²å›æ•°ï¼‰",
            color="é‡è¦åº¦ï¼ˆsplitï¼‰", color_continuous_scale="Oranges",
        )
        fig_split.update_layout(showlegend=False, yaxis={"categoryorder": "total ascending"})
        st.plotly_chart(fig_split, use_container_width=True)

    with st.expander("ğŸ“– ç‰¹å¾´é‡é‡è¦åº¦ã®è§£é‡ˆ"):
        st.markdown("""
**gainï¼ˆæƒ…å ±åˆ©å¾—ï¼‰**: ãã®ç‰¹å¾´é‡ãŒä½¿ã‚ã‚ŒãŸåˆ†å²ã§ã®æƒ…å ±åˆ©å¾—ã®åˆè¨ˆã€‚äºˆæ¸¬ã¸ã®å®Ÿè³ªçš„ãªè²¢çŒ®åº¦ã‚’è¡¨ã™ã€‚

**splitï¼ˆåˆ†å‰²å›æ•°ï¼‰**: ãã®ç‰¹å¾´é‡ãŒæœ¨å…¨ä½“ã§åˆ†å²ã«ä½¿ã‚ã‚ŒãŸå›æ•°ã€‚

ä¸€èˆ¬çš„ã« **gain** ãŒãƒ¢ãƒ‡ãƒ«ã¸ã®è²¢çŒ®ã‚’ã‚ˆã‚Šæ­£ç¢ºã«åæ˜ ã—ã¾ã™ã€‚
        """)

    # --- Tree visualization ---
    st.markdown("### æœ¨ã®æ§‹é€ ")
    sl_col1, sl_col2 = st.columns(2)
    with sl_col1:
        tree_index = st.slider(
            "è¡¨ç¤ºã™ã‚‹æœ¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0 = æœ€åˆã®æœ¨ï¼‰",
            min_value=0, max_value=n_estimators - 1, value=0,
            key="lgb_tree_idx",
        )

    try:
        dump = model.booster_.dump_model()
        trees = dump.get("tree_info", [])
        if tree_index < len(trees):
            actual_depth = _get_tree_depth(trees[tree_index].get("tree_structure", {}))
        else:
            actual_depth = 1
    except Exception as e:
        st.warning(f"æœ¨ã®æ§‹é€ ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        actual_depth = 1

    with sl_col2:
        if actual_depth > 1:
            display_depth = st.slider(
                "è¡¨ç¤ºã™ã‚‹åˆ†å²ã®æ·±ã•",
                min_value=1, max_value=actual_depth,
                value=min(3, actual_depth),
                key="lgb_display_depth",
            )
        else:
            display_depth = actual_depth
            st.caption(f"æœ¨ã®æ·±ã•: {actual_depth}ï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ä¸è¦ï¼‰")

    try:
        if graphviz is not None and tree_index < len(trees):
            source = _build_depth_limited_digraph(
                trees[tree_index],
                max_display_depth=display_depth,
                feature_names=feature_cols,
                precision=3,
            )
            st.caption(f"æœ¨ #{tree_index}ã€€å®Ÿéš›ã®æ·±ã•: {actual_depth}ã€€è¡¨ç¤ºæ·±ã•: {display_depth}")
            st.graphviz_chart(source, use_container_width=True)
        else:
            st.warning("graphvizãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    except Exception as e:
        st.warning(f"æ¨¹å½¢å›³ã®æç”»ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # --- Learning curves ---
    st.markdown("### å­¦ç¿’æ›²ç·š")
    metric_keys = list(evals_result.get("training", {}).keys())
    if metric_keys:
        mk = metric_keys[0]
        train_loss = evals_result["training"][mk]
        valid_loss = evals_result["valid_1"][mk]
        fig_lc = go.Figure()
        fig_lc.add_trace(go.Scatter(
            x=list(range(1, len(train_loss) + 1)), y=train_loss,
            mode="lines", name="è¨“ç·´", line=dict(color="blue"),
        ))
        fig_lc.add_trace(go.Scatter(
            x=list(range(1, len(valid_loss) + 1)), y=valid_loss,
            mode="lines", name="ãƒ†ã‚¹ãƒˆ", line=dict(color="orange"),
        ))
        fig_lc.update_layout(
            title=f"å­¦ç¿’æ›²ç·šï¼ˆ{mk}ï¼‰",
            xaxis_title="ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°å›æ•°",
            yaxis_title=mk,
        )
        st.plotly_chart(fig_lc, use_container_width=True)

    # --- Predicted vs Actual (regression) ---
    if is_regression:
        st.markdown("### äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤")
        fig_pred = go.Figure()
        fig_pred.add_trace(go.Scatter(
            x=y_test, y=y_pred, mode="markers",
            marker=dict(color="green", opacity=0.6), name="ãƒ‡ãƒ¼ã‚¿",
        ))
        min_val = min(float(np.min(y_test)), float(np.min(y_pred)))
        max_val = max(float(np.max(y_test)), float(np.max(y_pred)))
        fig_pred.add_trace(go.Scatter(
            x=[min_val, max_val], y=[min_val, max_val],
            mode="lines", line=dict(color="red", dash="dash"), name="å®Œå…¨äºˆæ¸¬",
        ))
        fig_pred.update_layout(title="äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤", xaxis_title="å®Ÿæ¸¬å€¤", yaxis_title="äºˆæ¸¬å€¤")
        st.plotly_chart(fig_pred, use_container_width=True)
